---
title: "Homework7"
author: "Kelin Zhong"
date: "2020/10/20"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Exercise 5.3.1

### Find the value of the normalizing constant for $g$, i.e., the constant C such that $C\int_{0}^{\infty}(2x^{\theta-1}+x^{\theta-\frac{1}{2}})e^{-x}dx=1$. Show that g is a mixture of Gamma distributions. Identify the component distributions and their weights in the mixture.

\[
\begin{aligned}
C\int_{0}^{\infty}(2x^{\theta-1}+x^{\theta-\frac{1}{2}})e^{-x}dx =& 1 \\
C\{2\int_{0}^{\infty}x^{\theta-1}e^{-x}dx + \int_{0}^{\infty}x^{\theta-\frac{1}{2}}e^{-x}dx\} =& 1 \\
2C\Gamma(\theta)+C\Gamma(\theta+\frac{1}{2})=1
\end{aligned}
\]
Hence, we obtain $C=\frac{1}{2\Gamma(\theta)+\Gamma(\theta+\frac{1}{2})}$
Notice that $x^{\theta-1}e^{-x}\sim \Gamma(\theta,1)$ and $x^{\theta-\frac{1}{2}}e^{-x}\sim \Gamma(\theta+\frac{1}{2},1)$. Therefore, g is a mixture of Gamma distributions with weights $w_{1} = \frac{2\Gamma(\theta)}{2\Gamma(\theta)+\Gamma(\theta+\frac{1}{2})}$ and $w_{2} = \frac{\Gamma(\theta+\frac{1}{2})}{2\Gamma(\theta)+\Gamma(\theta+\frac{1}{2})}$.



### Design a procedure(pseudo-code) to sample from $g$; implement it in an R function; draw a sample of size $n=10000$ using your function for at least one $\theta$ value; plot the kernel density estimation of $g$ from your sample and the true density in one figure.

The pseudo-code to sample from g is:
```
SampleFromg <- function(theta,weight1,weight2,number of samples){
  obtain weight of first gamma function
  obtain weight of second gamma function
  use rgamma function to generate the samples with weights

}

```
Here we use Inverse Transform Method with rgamma function in R.

```{R}
Sample_From_g <- function(theta,number){
  x <- theta
  w1 <- (2*gamma(x))/(2*gamma(x)+gamma(x+0.5))
  w2 <- gamma(x+0.5)/(2*gamma(x)+gamma(x+0.5))
  g_samples <- w1*rgamma(n = number,shape = x, scale = 1) + w2*rgamma(n = number,shape = x + 0.5, scale = 1)
  return(g_samples)
}
```
Let $\theta=10$ and we generate the samples with Sample_From_g function.
```{R}
n <- 10000
data1 <- Sample_From_g(10,n)
```
Then we plot the estimation of kernal density and the true density in one plot.
```{R}
g_est <- density(data1,n =10000)
g_true <- function(x){
  w1 <- (2*gamma(10))/(2*gamma(10)+gamma(10.5))
  w2 <- gamma(10.5)/(2*gamma(10)+gamma(10.5))
  g_t <- w1*dgamma(x,shape = 10, scale = 1) + w2*dgamma(x,shape = 10.5, scale = 1)
  return(g_t)
}

x1 <- g_est$x
y1 <- g_est$y
y1t <- g_true(x1)

library(ggplot2)
library(dplyr)

df1 <- tbl_df(data.frame(x1,y1,y1t))

ggplot(df1,aes(x = x1)) +
  geom_line(aes(y = y1,color = "Estimation")) +
  geom_line(aes(y = y1t, color = "True") )+
  labs(title = "Estimation vs True") 

```





### Design a procedure (pseudo-code) to use rejection sampling to sample from  $f$ using $g$ as the instrumental distribution. Overlay the estimated kernel density of a random sample generated by your procedure and  $f$.
Note that 
\[
x^{\theta-\frac{1}{2}}e^{-x}\le \sqrt{4+x}x^{\theta-1}e^{-x}\le (2x^{\theta-1}+x^{\theta-\frac{1}{2}})e^{-x}
\]
Suppose $C_{f}$ is a constant such that $C_{f}\int_{0}^{\infty}f(x)dx=1$ and $h(x)$ be the pdf of $\Gamma(\theta+\frac{1}{2},1)$. Based on the inequality above, $C_{f}<\frac{1}{\Gamma(\theta+\frac{1}{2})}$.Therefore
\[
\begin{aligned}
f(x) \le& \frac{1}{\Gamma(\theta+\frac{1}{2})}\sqrt{4+x}x^{\theta-1}e^{-x} \\
\le& M\frac{1}{2\Gamma(\theta)+\Gamma(\theta+\frac{1}{2})}(2x^{\theta-1}+x^{\theta-\frac{1}{2}})e^{-x} \\
=& Mg(x)
\end{aligned}
\]
where $M = \frac{2\Gamma(\theta)+\Gamma(\theta+\frac{1}{2})}{\Gamma(\theta+\frac{1}{2})}$.
Hence, we may obtain
\[
\frac{f(x)}{Mg(x)}\ge \frac{\Gamma(\theta+\frac{1}{2})}{2\Gamma(\theta)+\Gamma(\theta+\frac{1}{2})}\frac{\sqrt{4+x}}{2+x^{-\frac{1}{2}}}
\]



Hence, the envelope is $\alpha g(x)$.

The pseudo-code is
```
define function f

Reject_Sample_f <- function(number){

create a null numeric vector

for(i in 1:10*n){
sample 1 sample from g
sample 1 sample from unif(0,1)
define reject criteria
if u[i] <= criteria {put X[i] into the new vector;j+1;break}
else i+1
}
return new vector
}
```
Let $\theta = 10$
```{R}
f <- function(x){
  sqrt(4+x)*(x^9)*exp(-x)
}


Reject_Sample_f <- function(number){
  n <- number
  f_new <- rep(0,n)
  j <- 1
  while(j < n){
  for(i in 1:10*n)
  x <- Sample_From_g(10,1)
  u <- runif(1,0,1)
  cri <- (gamma(10.5) * sqrt(4 + x)) /
((2 * gamma(10) + gamma(10.5)) * (2 + x ^ (1/2)))
    if (u < cri){
    f_new[j] = x
    j <- j + 1
    break
    }
    if(i == 10*n) {
      j <- j
      }
  }
  
  return(f_new)
}

c1 <- 1 / integrate(f, 0, Inf)$value

Fsample <- Reject_Sample_f(10000)
xf <- density(Fsample, n = 10000)$x
yf <- density(Fsample, n = 10000)$y
yft <- f(xf)/c1

df2 <- tbl_df(data.frame(xf,yf,yft))

```

```
ggplot(df2,aes(x = xf)) +
  geom_line(aes(y = yf,color = "Estimation")) +
  geom_line(aes(y = yft, color = "True") )+
  labs(title = "Estimation vs True") 
```



## Exercise 6.3.1

### Consider again the normal mixture example, except that the parameters of the normal distributions are considered unknown. Suppose that prior for $\mu_{1}$ and $\mu_{2}$ are $N(0,10^{2})$, that the prior for $\frac{1}{\sigma_{1}^{2}}$ and $\frac{1}{\sigma_{1}^{2}}$ are $\Gamma(a,b)$ with shape $a=0.5$ and scale $b=10$.Further, all the priors are independent. Design an MCMC using the Gibbs sampling approach to estimate all 5 parameters. You may use the arms() function in package HI. Run your chain for sufficiently long and drop the burn-in period. Plot the histogram of the results for all the parameters.

From the question we know that $\mu_{1}|X,\mu_{2}\sim N(0,10^2)$,$\frac{1}{\sigma_{1}^{2}},\frac{1}{\sigma_{2}^{2}}\sim\Gamma(0.5,10)$.

Define the likelihood function in R
```{R}
mylike <- function(x,delta,mu1,mu2,sigma2_1,sigma2_2){
  prod(delta * dnorm(x,mu1,sqrt(sigma2_1))) + (1-delta) * dnorm(x,mu2,sqrt(sigma2_2))
}

```



```{R}
library(HI)

mymcmc <- function(niter,mu1.init,mu2.init,sigma2_1.init,sigma2_2.init,nburn = 100){
  
  
}


```

```{R}
delta <- 0.7 # true value to be estimated based on the data
n <- 100
set.seed(123)
u <- rbinom(n, prob = delta, size = 1)
x <- rnorm(n, ifelse(u == 1, 7, 10), 0.5)

```










